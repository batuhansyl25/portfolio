---
title: "Go + Redis: Designing Fast Queues for Real-Time Workloads"
date: "December 2024"
readTime: "8 min read"
---

Real-time execution is critical in Web3 applications. Users expect instant blockchain responses â€” but balancing performance and reliability at scale is no small task.

## Problem: High Latency & Memory Leaks
While building compute pipelines for the **OpenGPU** project, we ran into several issues:

- **High Latency:** Go's built-in channels couldn't keep up under burst loads.  
- **Memory Leaks:** Long-lived goroutines piled up, consuming memory.  
- **Scalability Limits:** Channels were not enough for thousands of concurrent jobs.

## Solution: Redis Streams + Go Worker Pool
We built a distributed queue system using **Redis Streams** and a **Go worker pool**:

```go
type WorkerPool struct {
    workers  int
    jobQueue chan Job
    quit     chan bool
    redis    *redis.Client
}

func (wp *WorkerPool) Start() {
    for i := 0; i < wp.workers; i++ {
        go wp.worker(i)
    }
}

func (wp *WorkerPool) worker(id int) {
    for {
        select {
        case job := <-wp.jobQueue:
            wp.processJob(job)
        case <-wp.quit:
            return
        }
    }
}
```

### Key Implementation Details

**Redis Streams Configuration:**
```go
// Create stream for job processing
XADD jobs * type "compute" payload "{...}" priority "high"

// Consumer group for load balancing
XGROUP CREATE jobs workers $ MKSTREAM
```

**Connection Pooling:**
```go
redis.NewClient(&redis.Options{
    PoolSize:     100,
    MinIdleConns: 10,
    MaxRetries:   3,
})
```

## Results ðŸš€
The performance improvements were significant:

- **85% Latency Reduction:** Average processing time dropped from 2.3s to 0.3s
- **Memory Usage:** 60% decrease, completely eliminated memory leaks
- **Throughput:** Now handles 10,000+ requests per second
- **Reliability:** 99.9% uptime with automatic failover

## Production Best Practices

### 1. Connection Pooling
Always use connection pooling for Redis to avoid connection overhead:

```go
client := redis.NewClient(&redis.Options{
    PoolSize:     runtime.NumCPU() * 10,
    MinIdleConns: 5,
    MaxRetries:   3,
    DialTimeout:  5 * time.Second,
})
```

### 2. Error Handling & Retry Logic
Implement robust error handling with exponential backoff:

```go
func (wp *WorkerPool) processJobWithRetry(job Job) error {
    maxRetries := 3
    for i := 0; i < maxRetries; i++ {
        if err := wp.processJob(job); err == nil {
            return nil
        }
        time.Sleep(time.Duration(i+1) * time.Second)
    }
    return wp.sendToDeadLetterQueue(job)
}
```

### 3. Monitoring & Observability
Track key metrics for production health:

- **Queue Depth:** Monitor pending jobs
- **Worker Health:** Track active vs idle workers
- **Error Rates:** Alert on high failure rates
- **Processing Time:** P95 and P99 latencies

### 4. Graceful Shutdown
Ensure clean shutdown to prevent data loss:

```go
func (wp *WorkerPool) Shutdown() {
    close(wp.quit)
    
    // Wait for workers to finish current jobs
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()
    
    for i := 0; i < wp.workers; i++ {
        select {
        case <-ctx.Done():
            return
        case <-time.After(100 * time.Millisecond):
            // Check if workers are done
        }
    }
}
```

## Lessons Learned

1. **Redis Streams** are perfect for ordered, persistent queues
2. **Worker pools** provide better control than unlimited goroutines
3. **Connection pooling** is crucial for high-throughput applications
4. **Monitoring** is essential for production reliability

This implementation now powers OpenGPU's backend, handling enterprise-grade workloads with thousands of concurrent users. The combination of Go's concurrency model and Redis's persistence creates a robust foundation for real-time Web3 applications.

---

*Want to see the full implementation? Check out the [OpenGPU repository](https://github.com/opengpu) for the complete source code.*
